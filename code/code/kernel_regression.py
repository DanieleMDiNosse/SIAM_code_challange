
#%%

import time
import pickle
import numpy as np
from amm import amm
from params import params
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from sklearn.metrics import r2_score
from sklearn.kernel_ridge import KernelRidge
from utils import calculate_cvar, calculate_log_returns

# Ignore future warnings
import warnings
warnings.simplefilter(action='ignore')

DATA_FOLDER = '../'

def target_4_opt(theta, params, ret_inf=False, full_output=True):
    '''
    Target function for the optimization.
    INPUT:
        - theta: np.array containing the weights of the portfolio
        - params: dict containing the parameters of the simulation
        - ret_inf: bool, if True when the constraint E[ r>zeta ] > 0.7 is not
            satisfied the output is np.inf; otherwise is np.nan
        - full_output: bool, whenever to include also the constraint in the output
    OUTPUT:
        - cvar = If the constraint E[ r>zeta ] > 0.7 is satisfied, return CVaR.
            Otherwise, the result is either np.nan or np.inf, according to ret_inf
        - constraint = E[ r>zeta ] - 0.7
    '''
    np.random.seed(params['seed']) #Fix the seed for the next operations

    #Initialize the pools
    Rx0 = params['Rx0']
    Ry0 = params['Ry0']
    phi = params['phi']
    pools = amm(Rx=Rx0, Ry=Ry0, phi=phi)

    #The amount to invest on each pool is given by the weight in theta mult by the total capital
    xs_0 = params['x_0']*theta
    # Swap and mint
    l = pools.swap_and_mint(xs_0)

    # Simulate 1000 paths of trading in the pools
    end_pools, Rx_t, Ry_t, v_t, event_type_t, event_direction_t =\
        pools.simulate(
            kappa=params['kappa'],
            p=params['p'],
            sigma=params['sigma'],
            T=params['T'],
            batch_size=params['batch_size'])
    # Compute the log returns
    log_ret = calculate_log_returns(xs_0, end_pools, l)

    constraint= len(log_ret[ log_ret>params['zeta'] ]) / len(log_ret) - 0.7
    cvar, var = calculate_cvar(log_ret) #If the constraint is satisfied, return CVaR
    return constraint, cvar, var

class KernelRidge_Warper():
    def __init__(self, options):
        from sklearn.kernel_ridge import KernelRidge
        self.krr = KernelRidge(**options)

    def fit(self, x, y):
        self.krr.fit(x, y)

    def score(self, x, y):
        return self.krr.score(x, y)
    
    def predict(self, x):
        if len(x.shape) == 1:
            x = x.reshape(1, -1)
        return self.krr.predict(x)

# Load the dataset
with open(f'{DATA_FOLDER}/rgs_output.pickle', 'rb') as f:
    dataset = pickle.load(f)

'''
I want to understand how many points are needed for a decent fit of the algorithm.
So, I take the dataset generated byb the rgs and I try to fit it with a kernel regression.
I iterate over the number of points in the dataset and I compute the R^2 score of the fit.
'''

To_test = [10, 16, 20, 32, 50, 64, 100, 200, 256, 500, 1000, 2000, 3000, 4000, 5000, 10000]

# Some variables for the optimization
options = {'maxiter': 1000, 'ftol': 1e-8}
constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x)-1}]
bounds_initial_dist = [(1e-5, 1) for i in range(params['N_pools'])]

# Prepare the grid for the hyperparameters grid search
Kernels = ['additive_chi2', 'linear', 'poly', 'polynomial',
           'rbf', 'laplacian', 'sigmoid', 'cosine']
Alphas = [1e-2, 1e-1, 1]

# Hyperparameters Oprimization via Grid Search
for kernel, alpha in [(k, a) for k in Kernels for a in Alphas]:
    print('Start experiment kernel:', kernel, 'alpha:', alpha)
    # Initialize the lists of results
    R2_list = list()
    Cvar_list = list()

    # Iterates over hte number of points in the dataset
    for n_points in tqdm(To_test[:-1]):
        print('Working on', n_points, 'points')
        # Select the points
        x_data = np.array(dataset['points'][:n_points])
        y_data = np.array(dataset['cvar'][:n_points])
        # Fit the model
        start = time.time()
        #print('Fitting the model...')
        krr = KernelRidge_Warper({'alpha':alpha, 'kernel':kernel})
        krr.fit(x_data, y_data)
        #print('Fitted after', time.time()-start, 'seconds')
        # Compute and save the R^2 score
        r2 = r2_score(y_data, krr.predict(x_data))
        R2_list.append( r2 )
        # Find the minimum and save it
        #start = time.time()
        #print('Minimizing the function...')
        result = minimize(lambda x: krr.predict(x), np.array([1/6]*6),
                        method='SLSQP', bounds=bounds_initial_dist,
                        constraints=constraints, options=options, tol=1e-8)
        #print('Minimized after', time.time()-start, 'seconds')
        #print(result.message)
        print('Total computational time:', time.time()-start, 'seconds')
        # Compute and save the CVaR
        cvar = target_4_opt(result.x ,params)[1]
        Cvar_list.append( cvar )
        print('CVaR value:', cvar)

    print('\n')
    print('R2_list:', R2_list)
    print('Cvar_list:', Cvar_list)

    # Plot the results
    '''plt.plot(To_test, R2_list)
    plt.title('R2 vs number of points')
    plt.show()

    plt.plot(To_test, Cvar_list)
    plt.title('CVaR vs number of points')
    plt.show()'''

    print('The best CVaR found is', np.min(Cvar_list))
    print('End experiment kernel:', kernel, 'alpha:', alpha)
    print('\n\n\n')

'''
An interesting result:
'kernel':'sigmoid'; n_points=20 -> cvar = 0.00439938363397633; but r2 = 0.001734082183825869

{'kernel': 'additive_chi2', 'alpha': 1, 'n_points':32} -> r2=0.7596775109196356; cvar=0.004198434235702302

Questo è il milgiore. Se non esce bene così, prova ad alzare leggermente il numero di punti
{'kernel': 'additive_chi2', 'alpha': 0.1, 'n_points':10} -> r2=0.9718366046737115; cvar=0.004286735437839644

Ancora più migliore di quello di prima
{'kernel': 'additive_chi2', 'alpha': 0.01, 'n_points':10} -> r2=0.9988363791595369; cvar=0.003912196770449521
{'kernel': 'additive_chi2', 'alpha': 0.01, 'n_points':32} -> r2=0.9976311296156791; cvar=0.003695553339743549

{'kernel': 'laplacian', 'alpha': 0.01, 'n_points':10} -> r2=0.9951105025711151; cvar=0.0038975350091437068
{, 'n_points':} -> r2=; cvar=
{, 'n_points':} -> r2=; cvar=
{, 'n_points':} -> r2=; cvar=
{, 'n_points':} -> r2=; cvar=
'''
